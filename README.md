# AutoGrad-Eng

based on Andrej Karpathy's lecture on the fundamentals of neural networks from the ground up

## Features
- Custom computation for backpropagation.
- Stochastic Gradient Descent (SGD) applied to all nodes.
- Visualizations for neural network computations.
- **Derivative Calculations**: Includes examples of calculating derivatives directly in the network using ValueNodes.
- **PyTorch Example**: Demonstrates how backpropagation concepts translate to PyTorch with paralell example.


## Additional Learning Resources
Here are some awesome practices related to topics covered in this project:

- [Google Colab Notebook](https://colab.research.google.com/drive/1FPTx1RXtBfc4MaTkf7viZZD4U2F9gtKN?usp=sharing)
- Also, check out Andrej Karpathy's course: [Zero to Hero](https://karpathy.ai/zero-to-hero.html)

#### Example Visualization
Below is an image illustrating the calculation of the loss function and setting gradients using backpropagation with graphviz:

![image](https://github.com/user-attachments/assets/576a1ebc-f0c4-4dd9-a772-e79a1b5ceac2)
